{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from dataset.cad_dataset import get_dataloader\n",
    "from config import ConfigAE\n",
    "from utils import ensure_dir\n",
    "from trainer import TrainerAE\n",
    "from trainer.loss import CADLoss\n",
    "from trainer.accuracy import CADAccuracy\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "#from cadlib.macro import EOS_IDX4\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import ast\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 82082\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "\n",
    "shuffled_df = pd.read_csv('selected_data.csv')\n",
    "# remove representations with length = 7\n",
    "shuffled_df = shuffled_df[shuffled_df[\"rep_len\"] > 7]\n",
    "# print(\"Computing lengths of representations...\")\n",
    "n_datasample = len(shuffled_df)\n",
    "print(\"Number of training samples:\", n_datasample)\n",
    "\n",
    "l = n_datasample//649\n",
    "seq_len = 32\n",
    "batch_size = 128\n",
    "# Split Data\n",
    "train_data = shuffled_df[:585*l].reset_index(drop=True)\n",
    "val_data = shuffled_df[585*l:617*l].reset_index(drop=True)\n",
    "test_data = shuffled_df[617*l:649*l].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_DIR = 'pics/'\n",
    "\n",
    "# Get ground truth labels from DeepCAD outputa\n",
    "class sketchDataset(Dataset):\n",
    "    # Takes a pandas dataframe input of image filenames under 'Name' and labels under 'Rep'\n",
    "    def __init__(self, df):\n",
    "        self.X = df['Name']\n",
    "        self.y = df['Rep']\n",
    "        # transform normalizes and prepares image for pretraind VGG network (\n",
    "        # norm Âµ and sigma from data used for VGG training)\n",
    "        self.transform = T.Compose([\n",
    "            T.Resize(256),\n",
    "            # T.CenterCrop(240),\n",
    "            T.ToTensor(),\n",
    "            T.Lambda(lambda tensor: ((-tensor + 1)>0).float()),\n",
    "            # T.Normalize(mean=np.mean([0.485, 0.456, 0.406]), std=np.mean([0.229, 0.224, 0.225]))\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        # Denotes the total number of samples\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Generates one sample of data\n",
    "        imageFileName = self.X[index]\n",
    "        label = ast.literal_eval(self.y[index])\n",
    "        pad_token = [3] + 16 * [-1]\n",
    "        label_pad = label + (60 - len(label)) * [pad_token]\n",
    "        y_label = torch.tensor(label_pad)\n",
    "\n",
    "        # print(imageFileName\n",
    "        imageFileName = \"\".join([\"0\" for i in range(8-len(str(imageFileName)))]) + str(imageFileName)\n",
    "        image = Image.open(IMG_DIR + imageFileName + '.png')\n",
    "        image = image.convert('L')\n",
    "        image = self.transform(image)\n",
    "\n",
    "        def expand(array, a):\n",
    "            # Define a kernel for convolution that checks for neighbors\n",
    "            # Define a kernel for convolution that checks for neighbors\n",
    "            kernel = torch.tensor([[1, 1, 1],\n",
    "                                [1, 0, 1],\n",
    "                                [1, 1, 1]], dtype=torch.float32)\n",
    "\n",
    "            # Apply convolution to the input tensor\n",
    "            convolved = torch.nn.functional.conv2d(array.unsqueeze(0), kernel.unsqueeze(0).unsqueeze(0), padding=1)\n",
    "\n",
    "            # Set positions with values greater than 0 to 1\n",
    "            return ((convolved > 0).float()[0]-image)*a + image\n",
    "        \n",
    "        alpha = [0.7**j for j in range(6)]\n",
    "        for i in range(len(alpha)):\n",
    "            image = expand(image, alpha[i])\n",
    "        return image, y_label[:, 0], y_label[:, 1:]\n",
    "\n",
    "traindata = sketchDataset(train_data)\n",
    "testdata = sketchDataset(test_data)\n",
    "valdata = sketchDataset(val_data)\n",
    "\n",
    "batch_size = 128\n",
    "train_dataloader = DataLoader(traindata, batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "test_dataloader = DataLoader(testdata, batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_dataloader = DataLoader(valdata, batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "\n",
    "def show_images(images, nmax=16):\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.imshow(make_grid((images.detach()[:]), nrow=4).permute(1, 2, 0))\n",
    "\n",
    "def show_batch(dl, nmax=16):\n",
    "    for images in dl:\n",
    "        show_images(images[0], 1)\n",
    "        print(images[0].shape)\n",
    "        break\n",
    "\n",
    "show_batch(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CNNEncoderVAE(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(CNNEncoderVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Encoder architecture\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=5, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=5, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(256, 512, kernel_size=7, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # Linear layers for mean and log variance\n",
    "        self.fc_mean = nn.Linear(8192, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(8192, latent_dim)\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        epsilon = torch.randn_like(std)\n",
    "        return mean + epsilon * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder forward pass\n",
    "        features = self.encoder(x)\n",
    "        batch_size = features.size(0)\n",
    "        features = features.view(batch_size, -1)  # Flatten the features\n",
    "        \n",
    "        # Calculate mean and log variance\n",
    "        mean = self.fc_mean(features)\n",
    "        logvar = self.fc_logvar(features)\n",
    "        \n",
    "        # Reparameterize to sample from the latent space\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        return z, mean, logvar\n",
    "\n",
    "\n",
    "class CNNDecoderVAE(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(CNNDecoderVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Decoder architecture\n",
    "        self.fc_mean = nn.Linear(latent_dim, 256)  # Linear layer for the mean\n",
    "        self.fc_logvar = nn.Linear(latent_dim, 256)  # Linear layer for the log variance\n",
    "        self.fc2 = nn.Linear(latent_dim, 8192)  # Reverse the dimensionality reduction from encoder\n",
    "        self.unpool = nn.MaxUnpool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear'),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=7, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear'),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=5, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear'),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=5, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear'),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear'),\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Sigmoid(),  # Use sigmoid activation to constrain output between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, z, mean, logvar):\n",
    "        # Decode from the latent space z\n",
    "        x = z.view(-1, 512, 4, 4)  # Reshape to match the last feature map size in the encoder\n",
    "        x = self.decoder(x)\n",
    "        return x, mean, logvar\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = CNNEncoderVAE(latent_dim)\n",
    "        self.decoder = CNNDecoderVAE(latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encode input to obtain mean, logvar, and latent representation\n",
    "        z, mean, logvar = self.encoder(x)\n",
    "        \n",
    "        # Decode from the latent representation\n",
    "        reconstructed_x, _, _ = self.decoder(z, mean, logvar)\n",
    "        \n",
    "        return reconstructed_x, mean, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE(8192)\n",
    "model.load_state_dict(torch.load(\"models/vae_sketch_19.pth\"))\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "loss = 0\n",
    "i = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels_cmd, labels_param in val_dataloader:\n",
    "        outputs = model(images)\n",
    "        if i==0:\n",
    "            fig, ax = plt.subplots(figsize=(8, 8))\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            ax.imshow(make_grid((images.detach()[:16]), nrow=4).permute(1, 2, 0))\n",
    "            fig2, ax2 = plt.subplots(figsize=(8, 8))\n",
    "            ax2.set_xticks([])\n",
    "            ax2.set_yticks([])\n",
    "            ax2.imshow(make_grid((outputs[0].detach()[:16]), nrow=4).permute(1, 2, 0))\n",
    "        loss += criterion(outputs[0], images)\n",
    "        print(i)\n",
    "        i+=1\n",
    "print(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
